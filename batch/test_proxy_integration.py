#!/usr/bin/env python3
"""
ãƒ—ãƒ­ã‚­ã‚·ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆ
å®Ÿè£…ã•ã‚ŒãŸProxyManagerã¨SessionManagerã®çµ±åˆå‹•ä½œã‚’ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import sys
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from batch.jobs.status_collection.aiohttp_loader import AiohttpHTMLLoader, ProxyManager
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_proxy_manager():
    """ProxyManagerã®åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== ProxyManageråŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ ===")
    
    config = {
        'use_proxy_rotation': True,
        'proxy_refresh_interval': 60,  # ãƒ†ã‚¹ãƒˆç”¨ã«çŸ­ç¸®
        'proxy_test_timeout': 5
    }
    
    proxy_manager = ProxyManager(config)
    
    # ãƒ—ãƒ­ã‚­ã‚·ãƒªã‚¹ãƒˆå–å¾—ãƒ†ã‚¹ãƒˆ
    logger.info("ğŸ“¡ ãƒ—ãƒ­ã‚­ã‚·ãƒªã‚¹ãƒˆå–å¾—ãƒ†ã‚¹ãƒˆ...")
    proxy_list = await proxy_manager.get_proxy_list()
    logger.info(f"âœ… å–å¾—ã—ãŸãƒ—ãƒ­ã‚­ã‚·æ•°: {len(proxy_list)}")
    
    if proxy_list:
        # æœ€åˆã®3ã¤ã®ãƒ—ãƒ­ã‚­ã‚·ã‚’ãƒ†ã‚¹ãƒˆ
        for i, proxy in enumerate(proxy_list[:3]):
            logger.info(f"ğŸ” ãƒ—ãƒ­ã‚­ã‚·ãƒ†ã‚¹ãƒˆ {i+1}: {proxy['host']}:{proxy['port']}")
            is_working = await proxy_manager.test_proxy(proxy)
            logger.info(f"{'âœ…' if is_working else 'âŒ'} ãƒ†ã‚¹ãƒˆçµæœ: {proxy['url']}")
    
    # ãƒ—ãƒ­ã‚­ã‚·ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ
    logger.info("\nğŸ”„ ãƒ—ãƒ­ã‚­ã‚·ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ...")
    for i in range(5):
        proxy = await proxy_manager.get_next_proxy()
        if proxy:
            logger.info(f"  {i+1}. {proxy['host']}:{proxy['port']} (source: {proxy.get('source', 'unknown')})")
        else:
            logger.warning(f"  {i+1}. ãƒ—ãƒ­ã‚­ã‚·å–å¾—å¤±æ•—")
    
    return proxy_manager

async def test_integrated_scraping():
    """çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
    logger.info("\n=== çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ ===")
    
    # ãƒ—ãƒ­ã‚­ã‚·ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æœ‰åŠ¹åŒ–è¨­å®š
    os.environ['FORCE_IMMEDIATE'] = 'true'  # ãƒ†ã‚¹ãƒˆç”¨ã«å¾…æ©Ÿæ™‚é–“ã‚’ã‚¹ã‚­ãƒƒãƒ—
    
    test_urls = [
        'http://httpbin.org/ip',  # IPã‚¢ãƒ‰ãƒ¬ã‚¹ç¢ºèª
        'http://httpbin.org/user-agent',  # User-Agentç¢ºèª
        'http://httpbin.org/headers',  # ãƒ˜ãƒƒãƒ€ãƒ¼ç¢ºèª
    ]
    
    async with AiohttpHTMLLoader() as loader:
        # ãƒ—ãƒ­ã‚­ã‚·ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æœ‰åŠ¹åŒ–
        loader.session_manager.proxy_manager = ProxyManager({
            'use_proxy_rotation': True,
            'proxy_refresh_interval': 60,
            'proxy_test_timeout': 5
        })
        
        for i, url in enumerate(test_urls):
            logger.info(f"\nğŸ“¡ ãƒ†ã‚¹ãƒˆ {i+1}: {url}")
            
            try:
                html = await loader.load_html(url)
                if html:
                    logger.info(f"âœ… å–å¾—æˆåŠŸ: {len(html)}æ–‡å­—")
                    # IPã‚¢ãƒ‰ãƒ¬ã‚¹æƒ…å ±ã‚’æŠ½å‡º
                    if 'httpbin.org/ip' in url:
                        import json
                        try:
                            ip_data = json.loads(html)
                            logger.info(f"ğŸŒ ä½¿ç”¨IP: {ip_data.get('origin', 'unknown')}")
                        except:
                            logger.info(f"ğŸ“„ ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {html[:200]}...")
                    else:
                        logger.info(f"ğŸ“„ ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {html[:200]}...")
                else:
                    logger.warning(f"âŒ å–å¾—å¤±æ•—: {url}")
                    
            except Exception as e:
                logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

async def test_proxy_failure_handling():
    """ãƒ—ãƒ­ã‚­ã‚·å¤±æ•—å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
    logger.info("\n=== ãƒ—ãƒ­ã‚­ã‚·å¤±æ•—å‡¦ç†ãƒ†ã‚¹ãƒˆ ===")
    
    config = {
        'use_proxy_rotation': True,
        'proxy_refresh_interval': 60,
        'proxy_test_timeout': 2  # çŸ­ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§å¤±æ•—ã‚’èª˜ç™º
    }
    
    proxy_manager = ProxyManager(config)
    
    # ç„¡åŠ¹ãªãƒ—ãƒ­ã‚­ã‚·ã‚’æ‰‹å‹•ã§è¿½åŠ 
    invalid_proxy = {
        'host': '192.168.1.999',
        'port': 8080,
        'type': 'http',
        'url': 'http://192.168.1.999:8080',
        'source': 'test'
    }
    
    proxy_manager.proxy_list.append(invalid_proxy)
    logger.info(f"ğŸ§ª ç„¡åŠ¹ãƒ—ãƒ­ã‚­ã‚·è¿½åŠ : {invalid_proxy['url']}")
    
    # å¤±æ•—ãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
    proxy_manager.mark_proxy_failed(invalid_proxy['url'])
    logger.info(f"âŒ ãƒ—ãƒ­ã‚­ã‚·å¤±æ•—ãƒãƒ¼ã‚¯: {invalid_proxy['url']}")
    
    # åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ã‚­ã‚·ãƒªã‚¹ãƒˆç¢ºèª
    available_proxies = await proxy_manager.get_proxy_list()
    logger.info(f"âœ… åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ã‚­ã‚·æ•°: {len(available_proxies)}")
    
    # å¤±æ•—ã—ãŸãƒ—ãƒ­ã‚­ã‚·ãŒé™¤å¤–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    failed_urls = [p['url'] for p in available_proxies if p['url'] == invalid_proxy['url']]
    if not failed_urls:
        logger.info("âœ… å¤±æ•—ãƒ—ãƒ­ã‚­ã‚·ãŒæ­£å¸¸ã«é™¤å¤–ã•ã‚Œã¾ã—ãŸ")
    else:
        logger.warning("âš ï¸ å¤±æ•—ãƒ—ãƒ­ã‚­ã‚·ãŒé™¤å¤–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("ğŸš€ ãƒ—ãƒ­ã‚­ã‚·ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # 1. ProxyManageråŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
        await test_proxy_manager()
        
        # 2. çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
        await test_integrated_scraping()
        
        # 3. ãƒ—ãƒ­ã‚­ã‚·å¤±æ•—å‡¦ç†ãƒ†ã‚¹ãƒˆ
        await test_proxy_failure_handling()
        
        logger.info("\nğŸ‰ å…¨ãƒ†ã‚¹ãƒˆå®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ç’°å¢ƒå¤‰æ•°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if 'FORCE_IMMEDIATE' in os.environ:
            del os.environ['FORCE_IMMEDIATE']

if __name__ == "__main__":
    asyncio.run(main())