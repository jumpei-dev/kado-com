"""
Cityheavenå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆ¦ç•¥

Cityheavenã‚µã‚¤ãƒˆã®æŒ‡ç¤ºæ›¸æº–æ‹ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†
"""

import aiohttp
from abc import ABC, abstractmethod
from bs4 import BeautifulSoup
from typing import Dict, List, Any, Optional
from datetime import datetime
import json

# Core models import
try:
    from ...core.models import CastStatus
except ImportError:
    try:
        from core.models import CastStatus
    except ImportError as e:
        print(f"CastStatus import failed: {e}")

# Local imports
try:
    from .html_loader import HTMLLoader
    from .aiohttp_loader import load_html_compatible
    from .cityheaven_parsers import CityheavenParserFactory
except ImportError:
    try:
        from html_loader import HTMLLoader
        from aiohttp_loader import load_html_compatible
        from cityheaven_parsers import CityheavenParserFactory
    except ImportError as e:
        print(f"Local imports failed: {e}")

try:
    from ..utils.datetime_utils import get_current_jst_datetime
except ImportError:
    try:
        from utils.datetime_utils import get_current_jst_datetime
    except ImportError:
        def get_current_jst_datetime():
            from datetime import datetime
            return datetime.now()

try:
    from ..utils.logging_utils import get_logger
except ImportError:
    try:
        from utils.logging_utils import get_logger
    except ImportError:
        def get_logger(name):
            import logging
            return logging.getLogger(name)

logger = get_logger(__name__)


class ScrapingStrategy(ABC):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆ¦ç•¥ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    async def scrape_working_status(self, business_name: str, business_id: str, base_url: str, use_local: bool = True) -> List[CastStatus]:
        """ç¨¼åƒã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’åé›†ã™ã‚‹æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰"""
        pass


class CityheavenStrategy(ScrapingStrategy):
    """Cityheavenã‚µã‚¤ãƒˆç”¨ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆ¦ç•¥ï¼ˆaiohttpä½¿ç”¨ï¼‰"""
    
    def __init__(self, use_local_html: bool = False, specific_file: Optional[str] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            use_local_html: ãƒ­ãƒ¼ã‚«ãƒ«HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆé–‹ç™ºç”¨ï¼‰
            specific_file: ç‰¹å®šã®HTMLãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŒ‡å®šï¼ˆDOMç¢ºèªãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰
        """
        self.use_local_html = use_local_html
        self.html_loader = HTMLLoader(use_local_html, specific_file)
        
        if use_local_html:
            logger.info("ğŸ”§ é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: ãƒ­ãƒ¼ã‚«ãƒ«HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
        else:
            logger.info("ğŸŒ æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰: aiohttpã§ãƒ©ã‚¤ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™")
    
    async def scrape_working_status(self, business_name: str, business_id: str, base_url: str, use_local: bool = True, dom_check_mode: bool = False) -> list[CastStatus]:
        """
        ç¨¼åƒçŠ¶æ³ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œï¼ˆæ™‚é–“åˆ¤å®šä¿®æ­£ç‰ˆï¼‰
        
        Args:
            dom_check_mode: è¿½åŠ åº—èˆ—DOMç¢ºèªãƒ¢ãƒ¼ãƒ‰ï¼ˆHTMLè©³ç´°å‡ºåŠ›ï¼‰
        """
        # URLç›´æ¥æŒ‡å®šï¼ˆDOMç¢ºèªãƒ¢ãƒ¼ãƒ‰ï¼‰ã®å ´åˆã¯å¼·åˆ¶çš„ã«ãƒªãƒ¢ãƒ¼ãƒˆå–å¾—
        if dom_check_mode and base_url and base_url.startswith('http'):
            use_local = False
            logger.info(f"ğŸ” DOMç¢ºèªãƒ¢ãƒ¼ãƒ‰: URLç›´æ¥æŒ‡å®šã®ãŸã‚ãƒªãƒ¢ãƒ¼ãƒˆå–å¾—ã‚’å¼·åˆ¶")
        
        if dom_check_mode:
            logger.info(f"ğŸ” è¿½åŠ åº—èˆ—DOMç¢ºèªãƒ¢ãƒ¼ãƒ‰: {business_name} - HTMLè©³ç´°å‡ºåŠ›æœ‰åŠ¹")
        else:
            logger.info(f"ğŸ“Š Cityheavenç¨¼åƒçŠ¶æ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {business_name}")
        
        # HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨å–å¾—æ™‚åˆ»ã‚’èª­ã¿è¾¼ã¿ï¼ˆä¿®æ­£ç‰ˆï¼‰
        if use_local:
            html_content, html_acquisition_time = await self.html_loader.load_html_content(
                business_name, business_id, None
            )
        else:
            html_content = await load_html_compatible(base_url)
            html_acquisition_time = get_current_jst_datetime()
        
        if not html_content:
            logger.error(f"HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {business_name}")
            return []
        
        # CityheavenParsersã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆDOMç¢ºèªãƒ¢ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°ã‚’æ¸¡ã™ï¼‰
        parser = CityheavenParserFactory.get_parser(business_id)
        cast_statuses = await parser.parse_cast_list(html_content, html_acquisition_time, dom_check_mode=dom_check_mode, business_id=business_id)
        
        if dom_check_mode:
            # DOMç¢ºèªãƒ¢ãƒ¼ãƒ‰ç”¨ã‚µãƒãƒªãƒ¼
            working_count = sum(1 for cast in cast_statuses if cast.is_working)
            on_shift_count = sum(1 for cast in cast_statuses if cast.on_shift)
            
            print(f"\nğŸ” ã€{business_name}ã€‘è¿½åŠ åº—èˆ—DOMç¢ºèªã‚µãƒãƒªãƒ¼")
            print("=" * 60)
            print(f"ç·ã‚­ãƒ£ã‚¹ãƒˆæ•°: {len(cast_statuses)}äºº")
            print(f"å‡ºå‹¤ä¸­: {on_shift_count}äºº ({on_shift_count/len(cast_statuses)*100:.1f}%)")
            print(f"ç¨¼åƒä¸­: {working_count}äºº ({working_count/on_shift_count*100:.1f}%)" if on_shift_count > 0 else "ç¨¼åƒä¸­: 0äºº")
            print("=" * 60)
        
        logger.info(f"âœ“ Cityheavenç¨¼åƒçŠ¶æ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {business_name}, {len(cast_statuses)} ä»¶")
        return cast_statuses
