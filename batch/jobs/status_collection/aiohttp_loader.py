"""Phase 1ÊîπËâØÁâà aiohttp HTML„É≠„Éº„ÉÄ„Éº
403„Ç®„É©„ÉºÂØæÁ≠ñÂº∑ÂåñÁâàÔºö„Çª„ÉÉ„Ç∑„Éß„É≥„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥„ÄÅCookie„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥„ÄÅ„É©„É≥„ÉÄ„É†ÈñìÈöîÊ©üËÉΩ‰ªò„Åç

ÂÖÉ„ÅÆaiohttp_loader.py„ÇíPhase 1ÂØæÁ≠ñ„ÅßÂº∑Âåñ
- 60ÂàÜ„Éô„Éº„Çπ„ÅÆ„É©„É≥„ÉÄ„É†ÈñìÈöîË™øÊï¥
- „Çª„ÉÉ„Ç∑„Éß„É≥„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥Ê©üËÉΩ
- Cookie„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥Ê©üËÉΩ
- Êã°ÂºµUser-Agent„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥
- 403„Ç®„É©„ÉºÂØæÁ≠ñ„ÅÆÂº∑Âåñ
"""

import asyncio
import aiohttp
import random
import logging
import time
import json
from typing import Optional, Dict, Any, List
from urllib.parse import urlparse
from pathlib import Path
import sys
from datetime import datetime, timedelta

# Ë®≠ÂÆöË™≠„ÅøËæº„Åø
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from utils.config import get_scraping_config
except ImportError:
    # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË®≠ÂÆö
    def get_scraping_config():
        return {
            'timeout': 30,
            'user_agents': [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            ],
            'min_delay': 0.5,
            'max_delay': 2.0,
            'retry_attempts': 3,
            'retry_delay': 3.0,
            'session_rotation': True,
            'session_lifetime': 1800,
            'cookie_persistence': True,
            'random_intervals': True,
            'interval_base_minutes': 60,
            'interval_variance_percent': 50,
            'random_headers': True,
            'random_referer': True
        }

logger = logging.getLogger(__name__)

class ProxyManager:
    """„Éó„É≠„Ç≠„Ç∑ÁÆ°ÁêÜ„ÇØ„É©„Çπ - „É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥Ê©üËÉΩ‰ªò„Åç"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.proxy_list = []
        self.current_proxy_index = 0
        self.failed_proxies = set()
        self.last_refresh_time = 0
        self.refresh_interval = config.get('proxy_refresh_interval', 3600)  # 1ÊôÇÈñì
        self.test_timeout = config.get('proxy_test_timeout', 10)
        
    async def get_proxy_list(self) -> List[Dict[str, Any]]:
        """„Éó„É≠„Ç≠„Ç∑„É™„Çπ„Éà„ÇíÂèñÂæóÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶Êõ¥Êñ∞Ôºâ"""
        current_time = time.time()
        
        # ÂàùÂõûÂèñÂæó„Åæ„Åü„ÅØÊõ¥Êñ∞ÈñìÈöî„ÇíÈÅé„Åé„ÅüÂ†¥Âêà
        if not self.proxy_list or (current_time - self.last_refresh_time) > self.refresh_interval:
            await self._refresh_proxy_list()
            
        # Â§±Êïó„Åó„Åü„Éó„É≠„Ç≠„Ç∑„ÇíÈô§Â§ñ
        available_proxies = [
            proxy for proxy in self.proxy_list 
            if proxy['url'] not in self.failed_proxies
        ]
        
        return available_proxies
        
    async def _refresh_proxy_list(self):
        """„Éó„É≠„Ç≠„Ç∑„É™„Çπ„Éà„ÇíÊõ¥Êñ∞"""
        logger.info("üì° „Éó„É≠„Ç≠„Ç∑„É™„Çπ„ÉàÊõ¥Êñ∞‰∏≠...")
        
        try:
            # Ë§áÊï∞„ÅÆ„Éó„É≠„Ç≠„Ç∑„ÇΩ„Éº„Çπ„Åã„ÇâÂèñÂæó
            new_proxies = []
            
            # ÁÑ°Êñô„Éó„É≠„Ç≠„Ç∑API 1: ProxyList
            try:
                proxies_1 = await self._fetch_from_proxylist_api()
                new_proxies.extend(proxies_1)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è ProxyList API „Ç®„É©„Éº: {e}")
                
            # ÁÑ°Êñô„Éó„É≠„Ç≠„Ç∑API 2: Free Proxy List
            try:
                proxies_2 = await self._fetch_from_free_proxy_api()
                new_proxies.extend(proxies_2)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Free Proxy API „Ç®„É©„Éº: {e}")
                
            # ÈáçË§áÈô§Âéª
            unique_proxies = {}
            for proxy in new_proxies:
                key = f"{proxy['host']}:{proxy['port']}"
                if key not in unique_proxies:
                    unique_proxies[key] = proxy
                    
            self.proxy_list = list(unique_proxies.values())
            self.last_refresh_time = time.time()
            
            logger.info(f"‚úÖ „Éó„É≠„Ç≠„Ç∑„É™„Çπ„ÉàÊõ¥Êñ∞ÂÆå‰∫Ü: {len(self.proxy_list)}ÂÄã")
            
        except Exception as e:
            logger.error(f"‚ùå „Éó„É≠„Ç≠„Ç∑„É™„Çπ„ÉàÊõ¥Êñ∞„Ç®„É©„Éº: {e}")
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Âü∫Êú¨ÁöÑ„Å™„Éó„É≠„Ç≠„Ç∑„É™„Çπ„Éà
            self.proxy_list = self._get_fallback_proxies()
            
    async def _fetch_from_proxylist_api(self) -> List[Dict[str, Any]]:
        """ProxyList API„Åã„Çâ„Éó„É≠„Ç≠„Ç∑„ÇíÂèñÂæó"""
        url = "https://www.proxy-list.download/api/v1/get?type=http"
        
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url) as response:
                if response.status == 200:
                    text = await response.text()
                    proxies = []
                    
                    for line in text.strip().split('\n'):
                        if ':' in line:
                            host, port = line.strip().split(':', 1)
                            proxies.append({
                                'host': host,
                                'port': int(port),
                                'type': 'http',
                                'url': f"http://{host}:{port}",
                                'source': 'proxylist'
                            })
                            
                    return proxies[:50]  # ÊúÄÂ§ß50ÂÄã
                    
        return []
        
    async def _fetch_from_free_proxy_api(self) -> List[Dict[str, Any]]:
        """Free Proxy API„Åã„Çâ„Éó„É≠„Ç≠„Ç∑„ÇíÂèñÂæó"""
        url = "https://api.proxyscrape.com/v2/?request=get&protocol=http&timeout=10000&country=all&ssl=all&anonymity=all"
        
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url) as response:
                if response.status == 200:
                    text = await response.text()
                    proxies = []
                    
                    for line in text.strip().split('\n'):
                        if ':' in line:
                            host, port = line.strip().split(':', 1)
                            try:
                                proxies.append({
                                    'host': host,
                                    'port': int(port),
                                    'type': 'http',
                                    'url': f"http://{host}:{port}",
                                    'source': 'proxyscrape'
                                })
                            except ValueError:
                                continue
                                
                    return proxies[:50]  # ÊúÄÂ§ß50ÂÄã
                    
        return []
        
    def _get_fallback_proxies(self) -> List[Dict[str, Any]]:
        """„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÁî®„ÅÆÂü∫Êú¨„Éó„É≠„Ç≠„Ç∑„É™„Çπ„Éà"""
        return [
            {'host': '8.210.83.33', 'port': 80, 'type': 'http', 'url': 'http://8.210.83.33:80', 'source': 'fallback'},
            {'host': '47.74.152.29', 'port': 8888, 'type': 'http', 'url': 'http://47.74.152.29:8888', 'source': 'fallback'},
            {'host': '20.111.54.16', 'port': 80, 'type': 'http', 'url': 'http://20.111.54.16:80', 'source': 'fallback'}
        ]
        
    async def get_next_proxy(self) -> Optional[Dict[str, Any]]:
        """Ê¨°„ÅÆ„Éó„É≠„Ç≠„Ç∑„ÇíÂèñÂæóÔºà„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥Ôºâ"""
        available_proxies = await self.get_proxy_list()
        
        if not available_proxies:
            logger.warning("‚ö†Ô∏è Âà©Áî®ÂèØËÉΩ„Å™„Éó„É≠„Ç≠„Ç∑„Åå„ÅÇ„Çä„Åæ„Åõ„Çì")
            return None
            
        # „É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥
        if self.current_proxy_index >= len(available_proxies):
            self.current_proxy_index = 0
            
        proxy = available_proxies[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(available_proxies)
        
        logger.debug(f"üîÑ „Éó„É≠„Ç≠„Ç∑„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥: {proxy['host']}:{proxy['port']}")
        return proxy
        
    def mark_proxy_failed(self, proxy_url: str):
        """„Éó„É≠„Ç≠„Ç∑„ÇíÂ§±Êïó„Å®„Åó„Å¶„Éû„Éº„ÇØ"""
        self.failed_proxies.add(proxy_url)
        logger.warning(f"‚ùå „Éó„É≠„Ç≠„Ç∑Â§±Êïó„Éû„Éº„ÇØ: {proxy_url}")
        
    def reset_failed_proxies(self):
        """Â§±Êïó„Éó„É≠„Ç≠„Ç∑„É™„Çπ„Éà„Çí„É™„Çª„ÉÉ„Éà"""
        self.failed_proxies.clear()
        logger.info("üîÑ Â§±Êïó„Éó„É≠„Ç≠„Ç∑„É™„Çπ„Éà„Çí„É™„Çª„ÉÉ„Éà")
        
    async def test_proxy(self, proxy: Dict[str, Any]) -> bool:
        """„Éó„É≠„Ç≠„Ç∑„ÅÆÂãï‰Ωú„ÉÜ„Çπ„Éà"""
        test_url = "http://httpbin.org/ip"
        
        try:
            timeout = aiohttp.ClientTimeout(total=self.test_timeout)
            connector = aiohttp.TCPConnector(ssl=False)
            
            async with aiohttp.ClientSession(
                timeout=timeout,
                connector=connector
            ) as session:
                proxy_url = f"http://{proxy['host']}:{proxy['port']}"
                
                async with session.get(
                    test_url,
                    proxy=proxy_url,
                    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                ) as response:
                    if response.status == 200:
                        logger.debug(f"‚úÖ „Éó„É≠„Ç≠„Ç∑„ÉÜ„Çπ„ÉàÊàêÂäü: {proxy['host']}:{proxy['port']}")
                        return True
                        
        except Exception as e:
            logger.debug(f"‚ùå „Éó„É≠„Ç≠„Ç∑„ÉÜ„Çπ„ÉàÂ§±Êïó: {proxy['host']}:{proxy['port']} - {e}")
            
        return False

class SessionManager:
    """„Çª„ÉÉ„Ç∑„Éß„É≥ÁÆ°ÁêÜ„ÇØ„É©„Çπ - „É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥Ê©üËÉΩ‰ªò„Åç"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.sessions: List[aiohttp.ClientSession] = []
        self.session_created_times: List[datetime] = []
        self.current_session_index = 0
        self.session_lifetime = config.get('session_lifetime', 1800)  # 30ÂàÜ
        self.cookie_jar_storage = {}
        
        # „Éó„É≠„Ç≠„Ç∑ÁÆ°ÁêÜ
        self.proxy_manager = None
        self.current_proxy = None
        if config.get('enable_proxy_rotation', False):
            self.proxy_manager = ProxyManager(config)
        
    async def get_session(self) -> aiohttp.ClientSession:
        """„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Å™„Çª„ÉÉ„Ç∑„Éß„É≥„ÇíÂèñÂæóÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥Ôºâ"""
        await self._cleanup_expired_sessions()
        
        if not self.sessions:
            await self._create_new_session()
            
        # „Çª„ÉÉ„Ç∑„Éß„É≥„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥
        if self.config.get('session_rotation', True):
            current_time = datetime.now()
            session_age = (current_time - self.session_created_times[self.current_session_index]).total_seconds()
            
            if session_age > self.session_lifetime:
                logger.info(f"üîÑ „Çª„ÉÉ„Ç∑„Éß„É≥„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥ÂÆüË°å (ÁµåÈÅéÊôÇÈñì: {session_age:.0f}Áßí)")
                await self._rotate_session()
                
        return self.sessions[self.current_session_index]
        
    async def _create_new_session(self) -> aiohttp.ClientSession:
        """Êñ∞„Åó„ÅÑ„Çª„ÉÉ„Ç∑„Éß„É≥„Çí‰ΩúÊàê"""
        timeout = aiohttp.ClientTimeout(total=self.config.get('timeout', 30))
        
        # Cookie Jar‰ΩúÊàêÔºàÊ∞∏Á∂öÂåñÂØæÂøúÔºâ
        cookie_jar = aiohttp.CookieJar()
        if self.config.get('cookie_persistence', True):
            # Êó¢Â≠ò„ÅÆCookie„ÇíÂæ©ÂÖÉ
            self._restore_cookies(cookie_jar)
            
        connector = aiohttp.TCPConnector(
            limit=100,
            limit_per_host=10,
            keepalive_timeout=30,
            enable_cleanup_closed=True,
            ssl=False  # SSLÊ§úË®º„ÇíÁ∑©Âíå
        )
        
        # „Éó„É≠„Ç≠„Ç∑Ë®≠ÂÆö„ÇíÂèñÂæó
        proxy_url = None
        if self.proxy_manager:
            try:
                proxy = await self.proxy_manager.get_next_proxy()
                if proxy:
                    proxy_url = proxy['url']
                    self.current_proxy = proxy  # ÁèæÂú®„ÅÆ„Éó„É≠„Ç≠„Ç∑„Çí‰øùÂ≠ò
                    logger.debug(f"üîÑ „Çª„ÉÉ„Ç∑„Éß„É≥„Å´„Éó„É≠„Ç≠„Ç∑Ë®≠ÂÆö: {proxy_url}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è „Éó„É≠„Ç≠„Ç∑ÂèñÂæó„Ç®„É©„Éº: {e}")
        
        session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            cookie_jar=cookie_jar,
            headers=self._get_base_headers()
        )
        
        # „Çª„ÉÉ„Ç∑„Éß„É≥„Å´„Éó„É≠„Ç≠„Ç∑ÊÉÖÂ†±„Çí‰øùÂ≠ò
        if proxy_url:
            session._proxy_url = proxy_url
        
        self.sessions.append(session)
        self.session_created_times.append(datetime.now())
        
        logger.info(f"üÜï Êñ∞„Åó„ÅÑ„Çª„ÉÉ„Ç∑„Éß„É≥‰ΩúÊàê (Á∑èÊï∞: {len(self.sessions)}, „Éó„É≠„Ç≠„Ç∑: {proxy_url or '„Å™„Åó'})")
        return session
    
    def get_current_proxy(self):
        """ÁèæÂú®„ÅÆ„Éó„É≠„Ç≠„Ç∑ÊÉÖÂ†±„ÇíÂèñÂæó"""
        return self.current_proxy
        
    async def _rotate_session(self):
        """„Çª„ÉÉ„Ç∑„Éß„É≥„Çí„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥"""
        # ÁèæÂú®„ÅÆ„Çª„ÉÉ„Ç∑„Éß„É≥„ÅÆCookie„Çí‰øùÂ≠ò
        if self.sessions and self.config.get('cookie_persistence', True):
            self._save_cookies(self.sessions[self.current_session_index].cookie_jar)
            
        # Âè§„ÅÑ„Çª„ÉÉ„Ç∑„Éß„É≥„ÇíÈñâ„Åò„Çã
        if self.sessions:
            await self.sessions[self.current_session_index].close()
            
        # Êñ∞„Åó„ÅÑ„Çª„ÉÉ„Ç∑„Éß„É≥„Çí‰ΩúÊàê
        await self._create_new_session()
        self.current_session_index = len(self.sessions) - 1
        
    async def _cleanup_expired_sessions(self):
        """ÊúüÈôêÂàá„Çå„Çª„ÉÉ„Ç∑„Éß„É≥„Çí„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó"""
        current_time = datetime.now()
        expired_indices = []
        
        for i, created_time in enumerate(self.session_created_times):
            if (current_time - created_time).total_seconds() > self.session_lifetime * 2:
                expired_indices.append(i)
                
        for i in reversed(expired_indices):
            if i < len(self.sessions):
                await self.sessions[i].close()
                del self.sessions[i]
                del self.session_created_times[i]
                
        if expired_indices:
            logger.info(f"üßπ ÊúüÈôêÂàá„Çå„Çª„ÉÉ„Ç∑„Éß„É≥ÂâäÈô§: {len(expired_indices)}ÂÄã")
            
    def _save_cookies(self, cookie_jar: aiohttp.CookieJar):
        """Cookie„Çí‰øùÂ≠ò"""
        try:
            cookies_data = []
            for cookie in cookie_jar:
                cookies_data.append({
                    'name': cookie.key,
                    'value': cookie.value,
                    'domain': cookie['domain'],
                    'path': cookie['path']
                })
            self.cookie_jar_storage['cookies'] = cookies_data
            logger.debug(f"üç™ Cookie‰øùÂ≠ò: {len(cookies_data)}ÂÄã")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Cookie‰øùÂ≠ò„Ç®„É©„Éº: {e}")
            
    def _restore_cookies(self, cookie_jar: aiohttp.CookieJar):
        """Cookie„ÇíÂæ©ÂÖÉ"""
        try:
            cookies_data = self.cookie_jar_storage.get('cookies', [])
            for cookie_data in cookies_data:
                cookie_jar.update_cookies({
                    cookie_data['name']: cookie_data['value']
                })
            if cookies_data:
                logger.debug(f"üç™ CookieÂæ©ÂÖÉ: {len(cookies_data)}ÂÄã")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è CookieÂæ©ÂÖÉ„Ç®„É©„Éº: {e}")
            
    def _get_base_headers(self) -> Dict[str, str]:
        """Âü∫Êú¨HTTP„Éò„ÉÉ„ÉÄ„Éº„ÇíÂèñÂæó"""
        return {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
            
    async def close_all(self):
        """ÂÖ®„Çª„ÉÉ„Ç∑„Éß„É≥„ÇíÈñâ„Åò„Çã"""
        for session in self.sessions:
            await session.close()
        self.sessions.clear()
        self.session_created_times.clear()
        
class AiohttpHTMLLoader:
    """Phase 1ÊîπËâØÁâà aiohttp HTML„É≠„Éº„ÉÄ„ÉºÔºàÊó¢Â≠ò„ÇØ„É©„ÇπÂêçÁ∂≠ÊåÅÔºâ"""
    
    def __init__(self):
        self.config = get_scraping_config()
        self.session_manager = SessionManager(self.config)
        self.last_request_time = 0
        self.request_count = 0
        
    async def __aenter__(self):
        """ÈùûÂêåÊúü„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Éû„Éç„Éº„Ç∏„É£„Éº„ÅÆÈñãÂßã"""
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ÈùûÂêåÊúü„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Éû„Éç„Éº„Ç∏„É£„Éº„ÅÆÁµÇ‰∫Ü"""
        await self.session_manager.close_all()
        
    def _get_random_user_agent(self) -> str:
        """„É©„É≥„ÉÄ„É†„Å™User-Agent„ÇíÂèñÂæóÔºàÊã°ÂºµÁâàÔºâ"""
        user_agents = self.config.get('user_agents', [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ])
        return random.choice(user_agents)
        
    def _get_base_headers(self) -> Dict[str, str]:
        """Âü∫Êú¨HTTP„Éò„ÉÉ„ÉÄ„Éº„ÇíÂèñÂæó"""
        return {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
    def _get_random_headers(self, url: str = "") -> Dict[str, str]:
        """„É©„É≥„ÉÄ„É†Âåñ„Åï„Çå„ÅüHTTP„Éò„ÉÉ„ÉÄ„Éº„ÇíÂèñÂæóÔºàPhase 1Âº∑ÂåñÁâàÔºâ"""
        headers = self._get_base_headers()
        headers['User-Agent'] = self._get_random_user_agent()
        
        # „É©„É≥„ÉÄ„É†„Éò„ÉÉ„ÉÄ„ÉºËøΩÂä†
        if self.config.get('random_headers', True):
            if random.random() < 0.3:
                headers['X-Requested-With'] = 'XMLHttpRequest'
                
            if random.random() < 0.4:
                headers['Sec-CH-UA'] = '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"'
                headers['Sec-CH-UA-Mobile'] = '?0'
                headers['Sec-CH-UA-Platform'] = '"Windows"'
                
        # „É©„É≥„ÉÄ„É†„É™„Éï„Ç°„É©„Éº
        if self.config.get('random_referer', True) and random.random() < 0.6 and url:
            referers = [
                'https://www.google.com/',
                'https://www.yahoo.co.jp/',
                'https://www.bing.com/',
                f"https://{urlparse(url).netloc}/"
            ]
            headers['Referer'] = random.choice(referers)
            
        return headers
        
    async def _calculate_random_delay(self) -> float:
        """„É©„É≥„ÉÄ„É†ÈñìÈöî„ÇíË®àÁÆóÔºàPhase 1: 60ÂàÜ„Éô„Éº„Çπ¬±50%Ôºâ"""
        if not self.config.get('random_intervals', True):
            return random.uniform(self.config.get('min_delay', 0.5), self.config.get('max_delay', 2.0))
            
        base_minutes = self.config.get('interval_base_minutes', 60)
        variance_percent = self.config.get('interval_variance_percent', 50)
        
        # ¬±50%„ÅÆÂ§âÂãï
        variance = base_minutes * (variance_percent / 100)
        random_minutes = random.uniform(base_minutes - variance, base_minutes + variance)
        
        # ÊúÄÂ∞è1ÂàÜ„ÄÅÊúÄÂ§ß120ÂàÜ„Å´Âà∂Èôê
        random_minutes = max(1, min(120, random_minutes))
        
        return random_minutes * 60  # Áßí„Å´Â§âÊèõ
        
    async def _random_delay(self):
        """„Çπ„Éû„Éº„ÉàÂæÖÊ©üÔºà„É™„ÇØ„Ç®„Çπ„ÉàÈñìÈöî„ÅÆÂãïÁöÑË™øÊï¥Ôºâ"""
        import os
        
        # Âº∑Âà∂Âç≥ÊôÇÂÆüË°å„É¢„Éº„Éâ„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ
        force_immediate = os.getenv('FORCE_IMMEDIATE', 'false').lower() == 'true'
        if force_immediate:
            logger.info("‚ö° Âº∑Âà∂Âç≥ÊôÇÂÆüË°å„É¢„Éº„Éâ - ÂÖ®„Å¶„ÅÆÂæÖÊ©üÊôÇÈñì„Çí„Çπ„Ç≠„ÉÉ„Éó")
            self.last_request_time = time.time()
            return
            
        current_time = time.time()
        
        # ÂâçÂõû„É™„ÇØ„Ç®„Çπ„Éà„Åã„Çâ„ÅÆÁµåÈÅéÊôÇÈñì
        if self.last_request_time > 0:
            elapsed = current_time - self.last_request_time
            
            # Áü≠ÊôÇÈñì„Åß„ÅÆÈÄ£Á∂ö„É™„ÇØ„Ç®„Çπ„Éà„ÇíÊ§úÂá∫
            if elapsed < 30:  # 30Áßí‰ª•ÂÜÖ
                self.request_count += 1
                if self.request_count > 3:  # 3Âõû‰ª•‰∏äÈÄ£Á∂ö
                    extra_delay = random.uniform(10, 30)  # ËøΩÂä†ÂæÖÊ©ü
                    logger.info(f"üõ°Ô∏è ÈÄ£Á∂ö„É™„ÇØ„Ç®„Çπ„ÉàÊ§úÂá∫ - ËøΩÂä†ÂæÖÊ©ü: {extra_delay:.1f}Áßí")
                    await asyncio.sleep(extra_delay)
                    self.request_count = 0
            else:
                self.request_count = 0
                
        # FORCE_IMMEDIATEÁí∞Â¢ÉÂ§âÊï∞„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØÂæÖÊ©ü„Çí„Çπ„Ç≠„ÉÉ„Éó
        if os.getenv('FORCE_IMMEDIATE', 'false').lower() == 'true':
            logger.info("‚ö° Âº∑Âà∂Âç≥ÊôÇÂÆüË°å„É¢„Éº„Éâ - „É©„É≥„ÉÄ„É†ÈñìÈöîÂæÖÊ©ü„Çí„Çπ„Ç≠„ÉÉ„Éó")
        elif self.config.get('random_intervals', True):
            # „É©„É≥„ÉÄ„É†ÈñìÈöîÂæÖÊ©ü
            delay = await self._calculate_random_delay()
            logger.info(f"‚è∞ „É©„É≥„ÉÄ„É†ÈñìÈöîÂæÖÊ©ü: {delay/60:.1f}ÂàÜ")
            await asyncio.sleep(delay)
        else:
            # ÈÄöÂ∏∏„ÅÆ„É©„É≥„ÉÄ„É†ÂæÖÊ©ü
            min_delay = self.config.get('min_delay', 0.5)
            max_delay = self.config.get('max_delay', 2.0)
            delay = random.uniform(min_delay, max_delay)
            logger.debug(f"‚è∞ ÈÄöÂ∏∏ÂæÖÊ©ü: {delay:.2f}Áßí")
            await asyncio.sleep(delay)
            
        self.last_request_time = current_time
        
    async def load_html(self, url: str, retries: Optional[int] = None) -> Optional[str]:
        """
        URL„Åã„ÇâHTML„ÇíÂèñÂæóÔºàPhase 1ÊîπËâØÁâàÔºâ
        
        Args:
            url: ÂèñÂæóÂØæË±°„ÅÆURL
            retries: „É™„Éà„É©„Ç§ÂõûÊï∞ÔºàNone„ÅÆÂ†¥Âêà„ÅØË®≠ÂÆö„Åã„ÇâÂèñÂæóÔºâ
            
        Returns:
            HTML„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Åæ„Åü„ÅØNoneÔºà„Ç®„É©„ÉºÊôÇÔºâ
        """
        if retries is None:
            retries = self.config.get('retry_attempts', 3)
            
        for attempt in range(retries + 1):
            try:
                # „Çπ„Éû„Éº„ÉàÂæÖÊ©ü
                if attempt > 0:
                    retry_delay = self.config.get('retry_delay', 3.0) * (attempt ** 2)  # ÊåáÊï∞„Éê„ÉÉ„ÇØ„Ç™„Éï
                    logger.info(f"üîÑ „É™„Éà„É©„Ç§ÂæÖÊ©ü: {retry_delay:.1f}Áßí")
                    await asyncio.sleep(retry_delay)
                else:
                    await self._random_delay()
                
                # „Çª„ÉÉ„Ç∑„Éß„É≥ÂèñÂæóÔºà„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥ÂØæÂøúÔºâ
                session = await self.session_manager.get_session()
                
                # „É©„É≥„ÉÄ„É†Âåñ„Åï„Çå„Åü„Éò„ÉÉ„ÉÄ„Éº„Åß„É™„ÇØ„Ç®„Çπ„Éà
                headers = self._get_random_headers(url)
                
                # „Éó„É≠„Ç≠„Ç∑Ë®≠ÂÆö„ÇíÂèñÂæó
                proxy_url = getattr(session, '_proxy_url', None)
                
                logger.info(f"üì° HTMLÂèñÂæóÈñãÂßã: {url} (Ë©¶Ë°å {attempt + 1}/{retries + 1})")
                logger.debug(f"üîß User-Agent: {headers['User-Agent'][:50]}...")
                if proxy_url:
                    logger.debug(f"üîÑ „Éó„É≠„Ç≠„Ç∑‰ΩøÁî®: {proxy_url}")
                
                # „Éó„É≠„Ç≠„Ç∑„Çí‰ΩøÁî®„Åó„Å¶„É™„ÇØ„Ç®„Çπ„Éà
                request_kwargs = {'headers': headers}
                if proxy_url:
                    request_kwargs['proxy'] = proxy_url
                
                async with session.get(url, **request_kwargs) as response:
                    if response.status == 200:
                        html_content = await response.text(encoding='utf-8')
                        
                        logger.info(f"‚úÖ HTMLÂèñÂæóÊàêÂäü: {url} ({len(html_content)}ÊñáÂ≠ó)")
                        return html_content
                        
                    elif response.status in [429, 503, 504]:  # „É¨„Éº„ÉàÂà∂Èôê„Éª„Çµ„Éº„Éê„ÉºÈÅéË≤†Ëç∑
                        logger.warning(f"üö´ „É¨„Éº„ÉàÂà∂ÈôêÊ§úÂá∫: HTTP {response.status} - {url}")
                        if attempt < retries:
                            wait_time = (attempt + 1) * 10  # „Çà„ÇäÈï∑„ÅÑÂæÖÊ©ü
                            logger.info(f"‚è∞ {wait_time}ÁßíÂæÖÊ©ü„Åó„Å¶„É™„Éà„É©„Ç§...")
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            logger.error(f"‚ùå „É™„Éà„É©„Ç§‰∏äÈôêÂà∞ÈÅî: {url}")
                            return None
                            
                    elif response.status in [403, 406]:  # „Ç¢„ÇØ„Çª„ÇπÊãíÂê¶
                        logger.warning(f"üö´ „Ç¢„ÇØ„Çª„ÇπÊãíÂê¶: HTTP {response.status} - {url}")
                        
                        # „Éó„É≠„Ç≠„Ç∑„ÅåÂéüÂõ†„ÅÆÂèØËÉΩÊÄß„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅÂ§±Êïó„Éû„Éº„ÇØ
                        if proxy_url and self.session_manager.proxy_manager:
                            self.session_manager.proxy_manager.mark_proxy_failed(proxy_url)
                            logger.info(f"‚ùå „Éó„É≠„Ç≠„Ç∑Â§±Êïó„Éû„Éº„ÇØ: {proxy_url}")
                        
                        if attempt < retries:
                            # „Çª„ÉÉ„Ç∑„Éß„É≥„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥Âº∑Âà∂ÂÆüË°å
                            logger.info("üîÑ 403„Ç®„É©„ÉºÂØæÁ≠ñ: „Çª„ÉÉ„Ç∑„Éß„É≥„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥ÂÆüË°å")
                            await self.session_manager._rotate_session()
                            continue
                        else:
                            logger.error(f"‚ùå „Ç¢„ÇØ„Çª„ÇπÊãíÂê¶„ÅåÁ∂ôÁ∂ö: {url}")
                            return None
                            
                    else:
                        logger.warning(f"‚ö†Ô∏è HTTP „Ç®„É©„Éº: {response.status} - {url}")
                        if attempt < retries:
                            continue
                        else:
                            logger.error(f"‚ùå HTTP „Ç®„É©„Éº„ÅåÁ∂ôÁ∂ö: {url}")
                            return None
                            
            except asyncio.TimeoutError:
                logger.warning(f"‚è∞ „Çø„Ç§„É†„Ç¢„Ç¶„Éà: {url} (Ë©¶Ë°å {attempt + 1}/{retries + 1})")
                if attempt < retries:
                    continue
                else:
                    logger.error(f"‚ùå „Çø„Ç§„É†„Ç¢„Ç¶„Éà„ÅåÁ∂ôÁ∂ö: {url}")
                    return None
                    
            except aiohttp.ClientError as e:
                logger.warning(f"üåê Êé•Á∂ö„Ç®„É©„Éº: {e} - {url} (Ë©¶Ë°å {attempt + 1}/{retries + 1})")
                
                # „Éó„É≠„Ç≠„Ç∑Êé•Á∂ö„Ç®„É©„Éº„ÅÆÂèØËÉΩÊÄß„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅÂ§±Êïó„Éû„Éº„ÇØ
                session = await self.session_manager.get_session()
                proxy_url = getattr(session, '_proxy_url', None)
                if proxy_url and self.session_manager.proxy_manager:
                    self.session_manager.proxy_manager.mark_proxy_failed(proxy_url)
                    logger.info(f"‚ùå „Éó„É≠„Ç≠„Ç∑Êé•Á∂ö„Ç®„É©„Éº„Åß„Éû„Éº„ÇØ: {proxy_url}")
                
                if attempt < retries:
                    # „Çª„ÉÉ„Ç∑„Éß„É≥„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥ÂÆüË°åÔºàÊñ∞„Åó„ÅÑ„Éó„É≠„Ç≠„Ç∑„ÇíÂèñÂæóÔºâ
                    await self.session_manager._rotate_session()
                    continue
                else:
                    logger.error(f"‚ùå Êé•Á∂ö„Ç®„É©„Éº„ÅåÁ∂ôÁ∂ö: {url}")
                    return None
                    
            except Exception as e:
                logger.error(f"‚ùå ‰∫àÊúü„Åó„Å™„ÅÑ„Ç®„É©„Éº: {e} - {url}")
                return None
                
        logger.error(f"‚ùå ÂÖ®„Å¶„ÅÆ„É™„Éà„É©„Ç§„ÅåÂ§±Êïó: {url}")
        return None

# ‰æøÂà©Èñ¢Êï∞ÔºàÊó¢Â≠ò„ÅÆÈñ¢Êï∞Âêç„ÇíÁ∂≠ÊåÅÔºâ
async def load_html_with_aiohttp(url: str) -> Optional[str]:
    """
    Phase 1ÊîπËâØÁâà aiohttp „Çí‰ΩøÁî®„Åó„Å¶HTML„ÇíÂèñÂæó„Åô„Çã‰æøÂà©Èñ¢Êï∞
    
    Args:
        url: ÂèñÂæóÂØæË±°„ÅÆURL
        
    Returns:
        HTML„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Åæ„Åü„ÅØNoneÔºà„Ç®„É©„ÉºÊôÇÔºâ
    """
    async with AiohttpHTMLLoader() as loader:
        return await loader.load_html(url)

# ‰∫íÊèõÊÄßÈñ¢Êï∞ÔºàÊó¢Â≠ò„ÅÆÈñ¢Êï∞Âêç„ÇíÁ∂≠ÊåÅÔºâ
async def load_html_compatible(url: str, use_aiohttp: bool = True) -> Optional[str]:
    """
    Phase 1ÊîπËâØÁâàaiohttp„Çí‰ΩøÁî®„Åó„Å¶HTML„ÇíÂèñÂæó
    
    Args:
        url: ÂèñÂæóÂØæË±°„ÅÆURL
        use_aiohttp: ‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÊÆã„Åï„Çå„Å¶„ÅÑ„Çã„Åå„ÄÅÂ∏∏„Å´Phase1 aiohttp„Çí‰ΩøÁî®
        
    Returns:
        HTML„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Åæ„Åü„ÅØNoneÔºà„Ç®„É©„ÉºÊôÇÔºâ
    """
    logger.info(f"üöÄ Phase1 aiohttp‰ΩøÁî®: {url}")
    return await load_html_with_aiohttp(url)
