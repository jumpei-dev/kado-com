"""
aiohttp ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿãƒ»è»½é‡HTMLãƒ­ãƒ¼ãƒ€ãƒ¼
ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–ã¨User-Agentãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ä»˜ã
"""

import asyncio
import aiohttp
import random
import logging
from typing import Optional, Dict, Any
from urllib.parse import urlparse
from pathlib import Path
import sys

# è¨­å®šèª­ã¿è¾¼ã¿
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from utils.config import get_scraping_config
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
    def get_scraping_config():
        return {
            'timeout': 30,
            'user_agents': [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            ],
            'min_delay': 0.5,
            'max_delay': 2.0,
            'retry_attempts': 3,
            'retry_delay': 3.0
        }

logger = logging.getLogger(__name__)

class AiohttpHTMLLoader:
    """aiohttp ã‚’ä½¿ç”¨ã—ãŸé«˜é€ŸHTMLãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self):
        self.config = get_scraping_config()
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®é–‹å§‹"""
        await self._create_session()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ‚äº†"""
        await self._close_session()
        
    async def _create_session(self):
        """aiohttp ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        timeout = aiohttp.ClientTimeout(total=self.config.get('timeout', 30))
        
        connector = aiohttp.TCPConnector(
            limit=100,  # ç·æ¥ç¶šæ•°åˆ¶é™
            limit_per_host=10,  # ãƒ›ã‚¹ãƒˆåˆ¥æ¥ç¶šæ•°åˆ¶é™
            keepalive_timeout=30,  # Keep-Alive ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            enable_cleanup_closed=True
        )
        
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers=self._get_base_headers()
        )
        
    async def _close_session(self):
        """aiohttp ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‰ã˜ã‚‹"""
        if self.session:
            await self.session.close()
            self.session = None
            
    def _get_random_user_agent(self) -> str:
        """ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentã‚’å–å¾—"""
        user_agents = self.config.get('user_agents', [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ])
        return random.choice(user_agents)
        
    def _get_base_headers(self) -> Dict[str, str]:
        """åŸºæœ¬HTTPãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        return {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
    def _get_random_headers(self) -> Dict[str, str]:
        """ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã•ã‚ŒãŸHTTPãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        headers = self._get_base_headers()
        headers['User-Agent'] = self._get_random_user_agent()
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãªè¦ç´ ã‚’è¿½åŠ ã—ã¦ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒ†ã‚£ãƒ³ã‚°å¯¾ç­–
        if random.random() < 0.3:  # 30%ã®ç¢ºç‡ã§è¿½åŠ ãƒ˜ãƒƒãƒ€ãƒ¼
            headers['X-Requested-With'] = 'XMLHttpRequest'
            
        if random.random() < 0.2:  # 20%ã®ç¢ºç‡ã§ãƒªãƒ•ã‚¡ãƒ©ãƒ¼è¿½åŠ 
            parsed_url = urlparse(headers.get('Referer', 'https://www.google.com/'))
            headers['Referer'] = f"{parsed_url.scheme}://{parsed_url.netloc}/"
            
        return headers
        
    async def _random_delay(self):
        """ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿã§ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–"""
        min_delay = self.config.get('min_delay', 0.5)
        max_delay = self.config.get('max_delay', 2.0)
        delay = random.uniform(min_delay, max_delay)
        
        logger.debug(f"â° ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ: {delay:.2f}ç§’")
        await asyncio.sleep(delay)
        
    async def load_html(self, url: str, retries: Optional[int] = None) -> Optional[str]:
        """
        URLã‹ã‚‰HTMLã‚’å–å¾—
        
        Args:
            url: å–å¾—å¯¾è±¡ã®URL
            retries: ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆNoneã®å ´åˆã¯è¨­å®šã‹ã‚‰å–å¾—ï¼‰
            
        Returns:
            HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯Noneï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
        """
        if retries is None:
            retries = self.config.get('retry_attempts', 3)
            
        if not self.session:
            await self._create_session()
            
        for attempt in range(retries + 1):
            try:
                # ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–: ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ
                if attempt > 0:
                    retry_delay = self.config.get('retry_delay', 3.0)
                    await asyncio.sleep(retry_delay * attempt)
                else:
                    await self._random_delay()
                
                # ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã•ã‚ŒãŸãƒ˜ãƒƒãƒ€ãƒ¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                headers = self._get_random_headers()
                
                logger.debug(f"ğŸ“¡ HTMLå–å¾—é–‹å§‹: {url} (è©¦è¡Œ {attempt + 1}/{retries + 1})")
                logger.debug(f"ğŸ”§ User-Agent: {headers['User-Agent'][:50]}...")
                
                async with self.session.get(url, headers=headers) as response:
                    if response.status == 200:
                        html_content = await response.text(encoding='utf-8')
                        
                        logger.info(f"âœ… HTMLå–å¾—æˆåŠŸ: {url} ({len(html_content)}æ–‡å­—)")
                        return html_content
                        
                    elif response.status in [429, 503, 504]:  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ã‚µãƒ¼ãƒãƒ¼éè² è·
                        logger.warning(f"ğŸš« ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡º: HTTP {response.status} - {url}")
                        if attempt < retries:
                            wait_time = (attempt + 1) * 5  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                            logger.info(f"â° {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            logger.error(f"âŒ ãƒªãƒˆãƒ©ã‚¤ä¸Šé™åˆ°é”: {url}")
                            return None
                            
                    elif response.status in [403, 406]:  # ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦
                        logger.warning(f"ğŸš« ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: HTTP {response.status} - {url}")
                        if attempt < retries:
                            # User-Agentã‚’å¤‰æ›´ã—ã¦ãƒªãƒˆãƒ©ã‚¤
                            continue
                        else:
                            logger.error(f"âŒ ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦ãŒç¶™ç¶š: {url}")
                            return None
                            
                    else:
                        logger.warning(f"âš ï¸ HTTP ã‚¨ãƒ©ãƒ¼: {response.status} - {url}")
                        if attempt < retries:
                            continue
                        else:
                            logger.error(f"âŒ HTTP ã‚¨ãƒ©ãƒ¼ãŒç¶™ç¶š: {url}")
                            return None
                            
            except asyncio.TimeoutError:
                logger.warning(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url} (è©¦è¡Œ {attempt + 1}/{retries + 1})")
                if attempt < retries:
                    continue
                else:
                    logger.error(f"âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒç¶™ç¶š: {url}")
                    return None
                    
            except aiohttp.ClientError as e:
                logger.warning(f"ğŸŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e} - {url} (è©¦è¡Œ {attempt + 1}/{retries + 1})")
                if attempt < retries:
                    continue
                else:
                    logger.error(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç¶™ç¶š: {url}")
                    return None
                    
            except Exception as e:
                logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e} - {url}")
                return None
                
        logger.error(f"âŒ å…¨ã¦ã®ãƒªãƒˆãƒ©ã‚¤ãŒå¤±æ•—: {url}")
        return None

# ä¾¿åˆ©é–¢æ•°
async def load_html_with_aiohttp(url: str) -> Optional[str]:
    """
    aiohttp ã‚’ä½¿ç”¨ã—ã¦HTMLã‚’å–å¾—ã™ã‚‹ä¾¿åˆ©é–¢æ•°
    
    Args:
        url: å–å¾—å¯¾è±¡ã®URL
        
    Returns:
        HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯Noneï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
    """
    async with AiohttpHTMLLoader() as loader:
        return await loader.load_html(url)

# aiohttpå°‚ç”¨ã®äº’æ›æ€§é–¢æ•°
async def load_html_compatible(url: str, use_aiohttp: bool = True) -> Optional[str]:
    """
    aiohttpã‚’ä½¿ç”¨ã—ã¦HTMLã‚’å–å¾—
    
    Args:
        url: å–å¾—å¯¾è±¡ã®URL
        use_aiohttp: äº’æ›æ€§ã®ãŸã‚æ®‹ã•ã‚Œã¦ã„ã‚‹ãŒã€å¸¸ã«aiohttpã‚’ä½¿ç”¨
        
    Returns:
        HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯Noneï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
    """
    logger.info(f"ğŸš€ aiohttpä½¿ç”¨: {url}")
    return await load_html_with_aiohttp(url)
