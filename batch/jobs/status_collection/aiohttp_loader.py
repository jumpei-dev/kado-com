"""Phase 1æ”¹è‰¯ç‰ˆ aiohttp HTMLãƒ­ãƒ¼ãƒ€ãƒ¼
403ã‚¨ãƒ©ãƒ¼å¯¾ç­–å¼·åŒ–ç‰ˆï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€Cookieãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ©ãƒ³ãƒ€ãƒ é–“éš”æ©Ÿèƒ½ä»˜ã

å…ƒã®aiohttp_loader.pyã‚’Phase 1å¯¾ç­–ã§å¼·åŒ–
- 60åˆ†ãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ³ãƒ€ãƒ é–“éš”èª¿æ•´
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½
- Cookieãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½
- æ‹¡å¼µUser-Agentãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- 403ã‚¨ãƒ©ãƒ¼å¯¾ç­–ã®å¼·åŒ–
"""

import asyncio
import aiohttp
import random
import logging
import time
import json
from typing import Optional, Dict, Any, List
from urllib.parse import urlparse
from pathlib import Path
import sys
from datetime import datetime, timedelta

# è¨­å®šèª­ã¿è¾¼ã¿
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from utils.config import get_scraping_config
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
    def get_scraping_config():
        return {
            'timeout': 30,
            'user_agents': [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            ],
            'min_delay': 0.5,
            'max_delay': 2.0,
            'retry_attempts': 3,
            'retry_delay': 3.0,
            'session_rotation': True,
            'session_lifetime': 1800,
            'cookie_persistence': True,
            'random_intervals': True,
            'interval_base_minutes': 60,
            'interval_variance_percent': 50,
            'random_headers': True,
            'random_referer': True
        }

logger = logging.getLogger(__name__)

class SessionManager:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚¯ãƒ©ã‚¹ - ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ä»˜ã"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.sessions: List[aiohttp.ClientSession] = []
        self.session_created_times: List[datetime] = []
        self.current_session_index = 0
        self.session_lifetime = config.get('session_lifetime', 1800)  # 30åˆ†
        self.cookie_jar_storage = {}
        
    async def get_session(self) -> aiohttp.ClientSession:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—ï¼ˆå¿…è¦ã«å¿œã˜ã¦ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        await self._cleanup_expired_sessions()
        
        if not self.sessions:
            await self._create_new_session()
            
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        if self.config.get('session_rotation', True):
            current_time = datetime.now()
            session_age = (current_time - self.session_created_times[self.current_session_index]).total_seconds()
            
            if session_age > self.session_lifetime:
                logger.info(f"ğŸ”„ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ (çµŒéæ™‚é–“: {session_age:.0f}ç§’)")
                await self._rotate_session()
                
        return self.sessions[self.current_session_index]
        
    async def _create_new_session(self) -> aiohttp.ClientSession:
        """æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        timeout = aiohttp.ClientTimeout(total=self.config.get('timeout', 30))
        
        # Cookie Jarä½œæˆï¼ˆæ°¸ç¶šåŒ–å¯¾å¿œï¼‰
        cookie_jar = aiohttp.CookieJar()
        if self.config.get('cookie_persistence', True):
            # æ—¢å­˜ã®Cookieã‚’å¾©å…ƒ
            self._restore_cookies(cookie_jar)
            
        connector = aiohttp.TCPConnector(
            limit=100,
            limit_per_host=10,
            keepalive_timeout=30,
            enable_cleanup_closed=True,
            ssl=False  # SSLæ¤œè¨¼ã‚’ç·©å’Œ
        )
        
        session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            cookie_jar=cookie_jar,
            headers=self._get_base_headers()
        )
        
        self.sessions.append(session)
        self.session_created_times.append(datetime.now())
        
        logger.info(f"ğŸ†• æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ (ç·æ•°: {len(self.sessions)})")
        return session
        
    async def _rotate_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®Cookieã‚’ä¿å­˜
        if self.sessions and self.config.get('cookie_persistence', True):
            self._save_cookies(self.sessions[self.current_session_index].cookie_jar)
            
        # å¤ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‰ã˜ã‚‹
        if self.sessions:
            await self.sessions[self.current_session_index].close()
            
        # æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        await self._create_new_session()
        self.current_session_index = len(self.sessions) - 1
        
    async def _cleanup_expired_sessions(self):
        """æœŸé™åˆ‡ã‚Œã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        current_time = datetime.now()
        expired_indices = []
        
        for i, created_time in enumerate(self.session_created_times):
            if (current_time - created_time).total_seconds() > self.session_lifetime * 2:
                expired_indices.append(i)
                
        for i in reversed(expired_indices):
            if i < len(self.sessions):
                await self.sessions[i].close()
                del self.sessions[i]
                del self.session_created_times[i]
                
        if expired_indices:
            logger.info(f"ğŸ§¹ æœŸé™åˆ‡ã‚Œã‚»ãƒƒã‚·ãƒ§ãƒ³å‰Šé™¤: {len(expired_indices)}å€‹")
            
    def _save_cookies(self, cookie_jar: aiohttp.CookieJar):
        """Cookieã‚’ä¿å­˜"""
        try:
            cookies_data = []
            for cookie in cookie_jar:
                cookies_data.append({
                    'name': cookie.key,
                    'value': cookie.value,
                    'domain': cookie['domain'],
                    'path': cookie['path']
                })
            self.cookie_jar_storage['cookies'] = cookies_data
            logger.debug(f"ğŸª Cookieä¿å­˜: {len(cookies_data)}å€‹")
        except Exception as e:
            logger.warning(f"âš ï¸ Cookieä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            
    def _restore_cookies(self, cookie_jar: aiohttp.CookieJar):
        """Cookieã‚’å¾©å…ƒ"""
        try:
            cookies_data = self.cookie_jar_storage.get('cookies', [])
            for cookie_data in cookies_data:
                cookie_jar.update_cookies({
                    cookie_data['name']: cookie_data['value']
                })
            if cookies_data:
                logger.debug(f"ğŸª Cookieå¾©å…ƒ: {len(cookies_data)}å€‹")
        except Exception as e:
            logger.warning(f"âš ï¸ Cookieå¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            
    def _get_base_headers(self) -> Dict[str, str]:
        """åŸºæœ¬HTTPãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        return {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
            
    async def close_all(self):
        """å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‰ã˜ã‚‹"""
        for session in self.sessions:
            await session.close()
        self.sessions.clear()
        self.session_created_times.clear()
        
class AiohttpHTMLLoader:
    """Phase 1æ”¹è‰¯ç‰ˆ aiohttp HTMLãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆæ—¢å­˜ã‚¯ãƒ©ã‚¹åç¶­æŒï¼‰"""
    
    def __init__(self):
        self.config = get_scraping_config()
        self.session_manager = SessionManager(self.config)
        self.last_request_time = 0
        self.request_count = 0
        
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®é–‹å§‹"""
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ‚äº†"""
        await self.session_manager.close_all()
        
    def _get_random_user_agent(self) -> str:
        """ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentã‚’å–å¾—ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        user_agents = self.config.get('user_agents', [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ])
        return random.choice(user_agents)
        
    def _get_base_headers(self) -> Dict[str, str]:
        """åŸºæœ¬HTTPãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        return {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
    def _get_random_headers(self, url: str = "") -> Dict[str, str]:
        """ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã•ã‚ŒãŸHTTPãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—ï¼ˆPhase 1å¼·åŒ–ç‰ˆï¼‰"""
        headers = self._get_base_headers()
        headers['User-Agent'] = self._get_random_user_agent()
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãƒ˜ãƒƒãƒ€ãƒ¼è¿½åŠ 
        if self.config.get('random_headers', True):
            if random.random() < 0.3:
                headers['X-Requested-With'] = 'XMLHttpRequest'
                
            if random.random() < 0.4:
                headers['Sec-CH-UA'] = '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"'
                headers['Sec-CH-UA-Mobile'] = '?0'
                headers['Sec-CH-UA-Platform'] = '"Windows"'
                
        # ãƒ©ãƒ³ãƒ€ãƒ ãƒªãƒ•ã‚¡ãƒ©ãƒ¼
        if self.config.get('random_referer', True) and random.random() < 0.6 and url:
            referers = [
                'https://www.google.com/',
                'https://www.yahoo.co.jp/',
                'https://www.bing.com/',
                f"https://{urlparse(url).netloc}/"
            ]
            headers['Referer'] = random.choice(referers)
            
        return headers
        
    async def _calculate_random_delay(self) -> float:
        """ãƒ©ãƒ³ãƒ€ãƒ é–“éš”ã‚’è¨ˆç®—ï¼ˆPhase 1: 60åˆ†ãƒ™ãƒ¼ã‚¹Â±50%ï¼‰"""
        if not self.config.get('random_intervals', True):
            return random.uniform(self.config.get('min_delay', 0.5), self.config.get('max_delay', 2.0))
            
        base_minutes = self.config.get('interval_base_minutes', 60)
        variance_percent = self.config.get('interval_variance_percent', 50)
        
        # Â±50%ã®å¤‰å‹•
        variance = base_minutes * (variance_percent / 100)
        random_minutes = random.uniform(base_minutes - variance, base_minutes + variance)
        
        # æœ€å°1åˆ†ã€æœ€å¤§120åˆ†ã«åˆ¶é™
        random_minutes = max(1, min(120, random_minutes))
        
        return random_minutes * 60  # ç§’ã«å¤‰æ›
        
    async def _random_delay(self):
        """ã‚¹ãƒãƒ¼ãƒˆå¾…æ©Ÿï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã®å‹•çš„èª¿æ•´ï¼‰"""
        import os
        
        # å¼·åˆ¶å³æ™‚å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
        force_immediate = os.getenv('FORCE_IMMEDIATE', 'false').lower() == 'true'
        if force_immediate:
            logger.info("âš¡ å¼·åˆ¶å³æ™‚å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ - å…¨ã¦ã®å¾…æ©Ÿæ™‚é–“ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            self.last_request_time = time.time()
            return
            
        current_time = time.time()
        
        # å‰å›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ã®çµŒéæ™‚é–“
        if self.last_request_time > 0:
            elapsed = current_time - self.last_request_time
            
            # çŸ­æ™‚é–“ã§ã®é€£ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ¤œå‡º
            if elapsed < 30:  # 30ç§’ä»¥å†…
                self.request_count += 1
                if self.request_count > 3:  # 3å›ä»¥ä¸Šé€£ç¶š
                    extra_delay = random.uniform(10, 30)  # è¿½åŠ å¾…æ©Ÿ
                    logger.info(f"ğŸ›¡ï¸ é€£ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¤œå‡º - è¿½åŠ å¾…æ©Ÿ: {extra_delay:.1f}ç§’")
                    await asyncio.sleep(extra_delay)
                    self.request_count = 0
            else:
                self.request_count = 0
                
        # FORCE_IMMEDIATEç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å¾…æ©Ÿã‚’ã‚¹ã‚­ãƒƒãƒ—
        if os.getenv('FORCE_IMMEDIATE', 'false').lower() == 'true':
            logger.info("âš¡ å¼·åˆ¶å³æ™‚å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ - ãƒ©ãƒ³ãƒ€ãƒ é–“éš”å¾…æ©Ÿã‚’ã‚¹ã‚­ãƒƒãƒ—")
        elif self.config.get('random_intervals', True):
            # ãƒ©ãƒ³ãƒ€ãƒ é–“éš”å¾…æ©Ÿ
            delay = await self._calculate_random_delay()
            logger.info(f"â° ãƒ©ãƒ³ãƒ€ãƒ é–“éš”å¾…æ©Ÿ: {delay/60:.1f}åˆ†")
            await asyncio.sleep(delay)
        else:
            # é€šå¸¸ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ
            min_delay = self.config.get('min_delay', 0.5)
            max_delay = self.config.get('max_delay', 2.0)
            delay = random.uniform(min_delay, max_delay)
            logger.debug(f"â° é€šå¸¸å¾…æ©Ÿ: {delay:.2f}ç§’")
            await asyncio.sleep(delay)
            
        self.last_request_time = current_time
        
    async def load_html(self, url: str, retries: Optional[int] = None) -> Optional[str]:
        """
        URLã‹ã‚‰HTMLã‚’å–å¾—ï¼ˆPhase 1æ”¹è‰¯ç‰ˆï¼‰
        
        Args:
            url: å–å¾—å¯¾è±¡ã®URL
            retries: ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆNoneã®å ´åˆã¯è¨­å®šã‹ã‚‰å–å¾—ï¼‰
            
        Returns:
            HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯Noneï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
        """
        if retries is None:
            retries = self.config.get('retry_attempts', 3)
            
        for attempt in range(retries + 1):
            try:
                # ã‚¹ãƒãƒ¼ãƒˆå¾…æ©Ÿ
                if attempt > 0:
                    retry_delay = self.config.get('retry_delay', 3.0) * (attempt ** 2)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    logger.info(f"ğŸ”„ ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿ: {retry_delay:.1f}ç§’")
                    await asyncio.sleep(retry_delay)
                else:
                    await self._random_delay()
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—ï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰
                session = await self.session_manager.get_session()
                
                # ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã•ã‚ŒãŸãƒ˜ãƒƒãƒ€ãƒ¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                headers = self._get_random_headers(url)
                
                logger.info(f"ğŸ“¡ HTMLå–å¾—é–‹å§‹: {url} (è©¦è¡Œ {attempt + 1}/{retries + 1})")
                logger.debug(f"ğŸ”§ User-Agent: {headers['User-Agent'][:50]}...")
                
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        html_content = await response.text(encoding='utf-8')
                        
                        logger.info(f"âœ… HTMLå–å¾—æˆåŠŸ: {url} ({len(html_content)}æ–‡å­—)")
                        return html_content
                        
                    elif response.status in [429, 503, 504]:  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ã‚µãƒ¼ãƒãƒ¼éè² è·
                        logger.warning(f"ğŸš« ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡º: HTTP {response.status} - {url}")
                        if attempt < retries:
                            wait_time = (attempt + 1) * 10  # ã‚ˆã‚Šé•·ã„å¾…æ©Ÿ
                            logger.info(f"â° {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            logger.error(f"âŒ ãƒªãƒˆãƒ©ã‚¤ä¸Šé™åˆ°é”: {url}")
                            return None
                            
                    elif response.status in [403, 406]:  # ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦
                        logger.warning(f"ğŸš« ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: HTTP {response.status} - {url}")
                        if attempt < retries:
                            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¼·åˆ¶å®Ÿè¡Œ
                            logger.info("ğŸ”„ 403ã‚¨ãƒ©ãƒ¼å¯¾ç­–: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ")
                            await self.session_manager._rotate_session()
                            continue
                        else:
                            logger.error(f"âŒ ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦ãŒç¶™ç¶š: {url}")
                            return None
                            
                    else:
                        logger.warning(f"âš ï¸ HTTP ã‚¨ãƒ©ãƒ¼: {response.status} - {url}")
                        if attempt < retries:
                            continue
                        else:
                            logger.error(f"âŒ HTTP ã‚¨ãƒ©ãƒ¼ãŒç¶™ç¶š: {url}")
                            return None
                            
            except asyncio.TimeoutError:
                logger.warning(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url} (è©¦è¡Œ {attempt + 1}/{retries + 1})")
                if attempt < retries:
                    continue
                else:
                    logger.error(f"âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒç¶™ç¶š: {url}")
                    return None
                    
            except aiohttp.ClientError as e:
                logger.warning(f"ğŸŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e} - {url} (è©¦è¡Œ {attempt + 1}/{retries + 1})")
                if attempt < retries:
                    continue
                else:
                    logger.error(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç¶™ç¶š: {url}")
                    return None
                    
            except Exception as e:
                logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e} - {url}")
                return None
                
        logger.error(f"âŒ å…¨ã¦ã®ãƒªãƒˆãƒ©ã‚¤ãŒå¤±æ•—: {url}")
        return None

# ä¾¿åˆ©é–¢æ•°ï¼ˆæ—¢å­˜ã®é–¢æ•°åã‚’ç¶­æŒï¼‰
async def load_html_with_aiohttp(url: str) -> Optional[str]:
    """
    Phase 1æ”¹è‰¯ç‰ˆ aiohttp ã‚’ä½¿ç”¨ã—ã¦HTMLã‚’å–å¾—ã™ã‚‹ä¾¿åˆ©é–¢æ•°
    
    Args:
        url: å–å¾—å¯¾è±¡ã®URL
        
    Returns:
        HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯Noneï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
    """
    async with AiohttpHTMLLoader() as loader:
        return await loader.load_html(url)

# äº’æ›æ€§é–¢æ•°ï¼ˆæ—¢å­˜ã®é–¢æ•°åã‚’ç¶­æŒï¼‰
async def load_html_compatible(url: str, use_aiohttp: bool = True) -> Optional[str]:
    """
    Phase 1æ”¹è‰¯ç‰ˆaiohttpã‚’ä½¿ç”¨ã—ã¦HTMLã‚’å–å¾—
    
    Args:
        url: å–å¾—å¯¾è±¡ã®URL
        use_aiohttp: äº’æ›æ€§ã®ãŸã‚æ®‹ã•ã‚Œã¦ã„ã‚‹ãŒã€å¸¸ã«Phase1 aiohttpã‚’ä½¿ç”¨
        
    Returns:
        HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯Noneï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
    """
    logger.info(f"ğŸš€ Phase1 aiohttpä½¿ç”¨: {url}")
    return await load_html_with_aiohttp(url)
