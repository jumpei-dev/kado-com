"""
ãƒãƒƒãƒå‡¦ç†ã®è¨­å®šç®¡ç†ã€‚
YAMLå½¢å¼ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ç’°å¢ƒå¤‰æ•°ã‚’ã‚µãƒãƒ¼ãƒˆã€‚

ğŸ“‹ è¨­å®šã®å„ªå…ˆé †ä½:
1. GitHub Actionså®Ÿè¡Œæ™‚: .github/workflows/status-collection.yml ãŒé©ç”¨
2. ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚: config/config.yml ã®è¨­å®šã‚’ä½¿ç”¨

âš ï¸  é‡è¦: GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã®åŒæœŸã‚’ä¿ã£ã¦ãã ã•ã„:
- ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: cron '0 */2 * * *' (2æ™‚é–“ã”ã¨)
- config.yml: status_collection_interval: 120åˆ†
- ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«: SchedulingConfig.status_collection_interval: 120åˆ†
"""

import os
import yaml
from dataclasses import dataclass, asdict
from typing import Dict, Any, Optional
from pathlib import Path

@dataclass
class DatabaseConfig:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š"""
    connection_string: str
    pool_size: int = 5
    max_overflow: int = 10
    pool_timeout: int = 30
    host: str = "localhost"
    port: int = 5432
    database: str = "postgres"
    user: str = "postgres"
    password: str = ""
    sslmode: str = "prefer"
    url: str = ""
    
    @classmethod
    def from_env(cls) -> 'DatabaseConfig':
        """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’ä½œæˆã™ã‚‹"""
        database_url = os.getenv('DATABASE_URL')
        if not database_url:
            raise ValueError("DATABASE_URLç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        return cls(
            connection_string=database_url,
            pool_size=int(os.getenv('DB_POOL_SIZE', 5)),
            max_overflow=int(os.getenv('DB_MAX_OVERFLOW', 10)),
            pool_timeout=int(os.getenv('DB_POOL_TIMEOUT', 30)),
            host=os.getenv('DB_HOST', 'localhost'),
            port=int(os.getenv('DB_PORT', 5432)),
            database=os.getenv('DB_NAME', 'postgres'),
            user=os.getenv('DB_USER', 'postgres'),
            password=os.getenv('DB_PASSWORD', ''),
            sslmode=os.getenv('DB_SSLMODE', 'prefer'),
            url=os.getenv('DB_URL', '')
        )
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'DatabaseConfig':
        """è¨­å®šè¾æ›¸ã‹ã‚‰è¨­å®šã‚’ä½œæˆã™ã‚‹"""
        db_config = config_dict
        
        # secret.ymlã‹ã‚‰æ©Ÿå¯†æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
        project_root = Path(__file__).parent.parent.parent
        secret_file = project_root / 'config' / 'secret.yml'
        
        secret_config = {}
        if secret_file.exists():
            try:
                with open(secret_file, 'r', encoding='utf-8') as f:
                    secret_data = yaml.safe_load(f)
                    # secret.ymlã®æ§‹é€ : database.url ã®å½¢å¼ã«å¯¾å¿œ
                    secret_config = secret_data.get('database', {})
            except Exception as e:
                print(f"secret.ymlèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ¥ç¶šæ–‡å­—åˆ—ã®å„ªå…ˆé †ä½: 1. secret.yml database.url, 2. ç’°å¢ƒå¤‰æ•°, 3. ã‚¨ãƒ©ãƒ¼
        if secret_config.get('url'):
            # secret.ymlã®database.urlãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆ
            connection_string = secret_config['url']
        else:
            # ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèª
            env_url = os.getenv('DATABASE_URL')
            if env_url:
                connection_string = env_url
            else:
                raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚secret.ymlã®database.urlã¾ãŸã¯DATABASE_URLç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        
        return cls(
            connection_string=connection_string,
            pool_size=db_config.get('pool_size', 5),
            max_overflow=db_config.get('max_overflow', 10),
            pool_timeout=db_config.get('pool_timeout', 30),
            host=db_config.get('host', 'localhost'),
            port=db_config.get('port', 5432),
            database=db_config.get('database', 'postgres'),
            user=db_config.get('user', 'postgres'),
            password=secret_config.get('password', db_config.get('password', '')),
            sslmode=db_config.get('sslmode', 'prefer'),
            url=secret_config.get('url', db_config.get('url', ''))
        )

@dataclass
class ScrapingConfig:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š"""
    timeout: int = 30
    max_concurrent: int = 10
    max_concurrent_businesses: int = 5  # åº—èˆ—ä¸¦è¡Œå‡¦ç†æ•°
    retry_attempts: int = 3
    retry_delay: float = 1.0
    delay_between_requests: float = 1.0
    user_agent: str = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    user_agents: list = None
    max_parallel_businesses: int = 3
    min_delay: float = 0.5
    max_delay: float = 2.0
    request_interval: float = 1.0
    use_aiohttp: bool = True
    connection_pooling: bool = True
    keep_alive: bool = True
    compress: bool = True
    session_rotation: bool = True  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æœ‰åŠ¹
    session_lifetime: int = 1800  # ã‚»ãƒƒã‚·ãƒ§ãƒ³æœ‰åŠ¹æœŸé–“ï¼ˆç§’ï¼‰30åˆ†
    cookie_persistence: bool = True  # Cookieæ°¸ç¶šåŒ–
    random_intervals: bool = True  # å®Œå…¨ãƒ©ãƒ³ãƒ€ãƒ é–“éš”
    interval_base_minutes: int = 60  # ãƒ™ãƒ¼ã‚¹é–“éš”ï¼ˆåˆ†ï¼‰
    interval_variance_percent: int = 50  # å¤‰å‹•å¹…ï¼ˆÂ±50%ï¼‰
    random_headers: bool = True  # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ©ãƒ³ãƒ€ãƒ åŒ–
    random_referer: bool = True  # ãƒªãƒ•ã‚¡ãƒ©ãƒ¼ãƒ©ãƒ³ãƒ€ãƒ åŒ–
    
    @classmethod
    def from_env(cls) -> 'ScrapingConfig':
        """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’ä½œæˆã™ã‚‹"""
        return cls(
            timeout=int(os.getenv('SCRAPING_TIMEOUT', 30)),
            max_concurrent=int(os.getenv('SCRAPING_MAX_CONCURRENT', 10)),
            max_concurrent_businesses=int(os.getenv('SCRAPING_MAX_CONCURRENT_BUSINESSES', 5)),
            retry_attempts=int(os.getenv('SCRAPING_RETRY_ATTEMPTS', 3)),
            retry_delay=float(os.getenv('SCRAPING_RETRY_DELAY', 1.0)),
            delay_between_requests=float(os.getenv('SCRAPING_DELAY_BETWEEN_REQUESTS', 1.0)),
            user_agent=os.getenv('SCRAPING_USER_AGENT', cls.user_agent),
            user_agents=None,
            max_parallel_businesses=int(os.getenv('SCRAPING_MAX_PARALLEL_BUSINESSES', 3)),
            min_delay=float(os.getenv('SCRAPING_MIN_DELAY', 0.5)),
            max_delay=float(os.getenv('SCRAPING_MAX_DELAY', 2.0)),
            request_interval=float(os.getenv('SCRAPING_REQUEST_INTERVAL', 1.0)),
            use_aiohttp=os.getenv('SCRAPING_USE_AIOHTTP', 'true').lower() == 'true',
            connection_pooling=os.getenv('SCRAPING_CONNECTION_POOLING', 'true').lower() == 'true',
            keep_alive=os.getenv('SCRAPING_KEEP_ALIVE', 'true').lower() == 'true',
            compress=os.getenv('SCRAPING_COMPRESS', 'true').lower() == 'true',
            session_rotation=os.getenv('SCRAPING_SESSION_ROTATION', 'true').lower() == 'true',
            session_lifetime=int(os.getenv('SCRAPING_SESSION_LIFETIME', 1800)),
            cookie_persistence=os.getenv('SCRAPING_COOKIE_PERSISTENCE', 'true').lower() == 'true',
            random_intervals=os.getenv('SCRAPING_RANDOM_INTERVALS', 'true').lower() == 'true',
            interval_base_minutes=int(os.getenv('SCRAPING_INTERVAL_BASE_MINUTES', 60)),
            interval_variance_percent=int(os.getenv('SCRAPING_INTERVAL_VARIANCE_PERCENT', 50)),
            random_headers=os.getenv('SCRAPING_RANDOM_HEADERS', 'true').lower() == 'true',
            random_referer=os.getenv('SCRAPING_RANDOM_REFERER', 'true').lower() == 'true'
        )

@dataclass
class SchedulingConfig:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¨­å®š
    
    âš ï¸  GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã®åŒæœŸå¿…é ˆ:
    - status_collection_interval: 120åˆ† = GitHub Actions cron '0 */2 * * *'
    - config.yml scheduling.status_collection_interval: 120åˆ†ã¨çµ±ä¸€
    """
    status_collection_interval: int = 120  # åˆ†ï¼ˆ2æ™‚é–“é–“éš”ï¼‰- GitHub Actionsã¨çµ±ä¸€
    history_calculation_hour: int = 12    # ç¨¼åƒç‡è¨ˆç®—å®Ÿè¡Œæ™‚åˆ»
    history_calculation_minute: int = 0   # ç¨¼åƒç‡è¨ˆç®—å®Ÿè¡Œåˆ†
    max_concurrent_businesses: int = 5        # åº—èˆ—ä¸¦è¡Œå‡¦ç†æ•°
    max_concurrent_working_rate: int = 10     # ç¨¼åƒç‡è¨ˆç®—ä¸¦è¡Œå‡¦ç†æ•°
    health_check_interval: int = 15       # åˆ†
    cleanup_time_hour: int = 2            # åˆå‰2æ™‚
    cleanup_time_minute: int = 0
    misfire_grace_time_status: int = 300  # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åé›†ã®çŒ¶äºˆæ™‚é–“ï¼ˆç§’ï¼‰
    misfire_grace_time_history: int = 600 # å±¥æ­´è¨ˆç®—ã®çŒ¶äºˆæ™‚é–“ï¼ˆç§’ï¼‰
    max_log_retention_days: int = 30      # ãƒ­ã‚°ä¿æŒæ—¥æ•°
    business_hours_buffer: int = 0        # å–¶æ¥­æ™‚é–“ãƒãƒƒãƒ•ã‚¡ï¼ˆåˆ†ï¼‰
    
    @classmethod
    def from_env(cls) -> 'SchedulingConfig':
        """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’ä½œæˆã™ã‚‹"""
        return cls(
            status_collection_interval=int(os.getenv('STATUS_COLLECTION_INTERVAL', 120)),  # GitHub Actionsã¨çµ±ä¸€
            history_calculation_hour=int(os.getenv('HISTORY_CALCULATION_HOUR', 12)),
            history_calculation_minute=int(os.getenv('HISTORY_CALCULATION_MINUTE', 0)),
            max_concurrent_businesses=int(os.getenv('MAX_CONCURRENT_BUSINESSES', 5)),
            max_concurrent_working_rate=int(os.getenv('MAX_CONCURRENT_WORKING_RATE', 10)),
            health_check_interval=int(os.getenv('HEALTH_CHECK_INTERVAL', 15)),
            cleanup_time_hour=int(os.getenv('CLEANUP_TIME_HOUR', 2)),
            cleanup_time_minute=int(os.getenv('CLEANUP_TIME_MINUTE', 0)),
            misfire_grace_time_status=int(os.getenv('MISFIRE_GRACE_TIME_STATUS', 300)),
            misfire_grace_time_history=int(os.getenv('MISFIRE_GRACE_TIME_HISTORY', 600)),
            max_log_retention_days=int(os.getenv('MAX_LOG_RETENTION_DAYS', 30)),
            business_hours_buffer=int(os.getenv('BUSINESS_HOURS_BUFFER', 0))
        )

@dataclass
class LoggingConfig:
    """Logging configuration settings."""
    level: str = "INFO"
    log_dir: str = "logs"
    max_file_size: int = 10 * 1024 * 1024  # 10MB
    backup_count: int = 5
    format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    @classmethod
    def from_env(cls) -> 'LoggingConfig':
        """Create configuration from environment variables."""
        return cls(
            level=os.getenv('LOG_LEVEL', 'INFO'),
            log_dir=os.getenv('LOG_DIR', 'logs'),
            max_file_size=int(os.getenv('LOG_MAX_FILE_SIZE', 10 * 1024 * 1024)),
            backup_count=int(os.getenv('LOG_BACKUP_COUNT', 5)),
            format=os.getenv('LOG_FORMAT', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        )

@dataclass
class BatchConfig:
    """ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒå‡¦ç†è¨­å®š"""
    database: DatabaseConfig
    scraping: ScrapingConfig
    scheduling: SchedulingConfig
    logging: LoggingConfig
    environment: str = "development"
    
    @classmethod
    def from_env(cls) -> 'BatchConfig':
        """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’ä½œæˆã™ã‚‹"""
        return cls(
            database=DatabaseConfig.from_env(),
            scraping=ScrapingConfig.from_env(),
            scheduling=SchedulingConfig.from_env(),
            logging=LoggingConfig.from_env(),
            environment=os.getenv('BATCH_ENVIRONMENT', 'development')
        )
    
    @classmethod
    def from_yaml(cls, config_path: str) -> 'BatchConfig':
        """YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€"""
        path = Path(config_path)
        if not path.exists():
            raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        
        with open(path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        # ç’°å¢ƒå¤‰æ•°ã®å±•é–‹
        data = cls._expand_env_vars(data)
        
        # schedulingã®ç‰¹åˆ¥ãªå‡¦ç†
        scheduling_data = data.get('scheduling', {})
        if 'misfire_grace_time' in scheduling_data:
            misfire_grace_time = scheduling_data['misfire_grace_time']
            scheduling_data['misfire_grace_time_status'] = misfire_grace_time.get('status_collection', 300)
            scheduling_data['misfire_grace_time_history'] = misfire_grace_time.get('status_history', 600)
            del scheduling_data['misfire_grace_time']
        
        return cls(
            database=DatabaseConfig.from_dict(data.get('database', {})),
            scraping=ScrapingConfig(**data.get('scraping', {})),
            scheduling=SchedulingConfig(**scheduling_data),
            logging=LoggingConfig(**data.get('logging', {})),
            environment=data.get('environment', 'development')
        )
    
    @classmethod
    def from_file(cls, config_path: str) -> 'BatchConfig':
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆYAMLå½¢å¼ï¼‰"""
        return cls.from_yaml(config_path)
    
    @classmethod 
    def _expand_env_vars(cls, config_dict: Dict[str, Any]) -> Dict[str, Any]:
        """è¨­å®šå†…ã®ç’°å¢ƒå¤‰æ•°ã‚’å±•é–‹ã™ã‚‹ï¼ˆ${VAR_NAME}å½¢å¼ï¼‰"""
        if isinstance(config_dict, dict):
            return {
                key: cls._expand_env_vars(value)
                for key, value in config_dict.items()
            }
        elif isinstance(config_dict, list):
            return [cls._expand_env_vars(item) for item in config_dict]
        elif isinstance(config_dict, str):
            # ${VAR_NAME}å½¢å¼ã®ç’°å¢ƒå¤‰æ•°ã‚’å±•é–‹
            import re
            def replace_env_var(match):
                var_name = match.group(1)
                return os.getenv(var_name, match.group(0))
            return re.sub(r'\$\{([^}]+)\}', replace_env_var, config_dict)
        else:
            return config_dict
    
    def to_yaml(self, config_path: str):
        """è¨­å®šã‚’YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        path = Path(config_path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        config_dict = {
            'database': {
                'host': getattr(self.database, 'host', 'localhost'),
                'port': getattr(self.database, 'port', 5432),
                'database': getattr(self.database, 'database', 'kado'),
                'user': getattr(self.database, 'user', 'postgres'),
                'password': getattr(self.database, 'password', 'password'),
            },
            'scraping': {
                'timeout': self.scraping.timeout,
                'max_concurrent': self.scraping.max_concurrent,
                'max_concurrent_businesses': self.scraping.max_concurrent_businesses,
                'retry_attempts': self.scraping.retry_attempts,
                'retry_delay': self.scraping.retry_delay,
                'user_agent': self.scraping.user_agent
            },
            'scheduling': {
                'status_collection_interval': self.scheduling.status_collection_interval,
                'history_calculation_hour': self.scheduling.history_calculation_hour,
                'history_calculation_minute': self.scheduling.history_calculation_minute,
                'max_concurrent_businesses': self.scheduling.max_concurrent_businesses,
                'max_concurrent_working_rate': self.scheduling.max_concurrent_working_rate,
                'health_check_interval': self.scheduling.health_check_interval,
                'cleanup_time_hour': self.scheduling.cleanup_time_hour,
                'cleanup_time_minute': self.scheduling.cleanup_time_minute
            },
            'logging': {
                'level': self.logging.level,
                'log_dir': self.logging.log_dir,
                'max_file_size': self.logging.max_file_size,
                'backup_count': self.logging.backup_count
            },
            'environment': self.environment
        }
        
        with open(path, 'w', encoding='utf-8') as f:
            yaml.dump(config_dict, f, allow_unicode=True, default_flow_style=False)
    
    @property
    def is_development(self) -> bool:
        """é–‹ç™ºç’°å¢ƒã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.environment.lower() == 'development'
    
    @property
    def is_production(self) -> bool:
        """æœ¬ç•ªç’°å¢ƒã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.environment.lower() == 'production'
    
    def to_dict(self) -> Dict[str, Any]:
        """è¨­å®šã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return asdict(self)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_config: Optional[BatchConfig] = None

def get_config() -> BatchConfig:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _config
    if _config is None:
        _config = load_config_for_environment()
    return _config

def set_config(config: BatchConfig):
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¨­å®š"""
    global _config
    _config = config

def load_config_from_file(config_path: str):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«è¨­å®š"""
    global _config
    _config = BatchConfig.from_file(config_path)

def load_config_for_environment(environment: str = None) -> BatchConfig:
    """çµ±åˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€"""
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ config.yml ã‚’èª­ã¿è¾¼ã¿
    project_root = Path(__file__).parent.parent.parent
    config_file = project_root / 'config' / 'config.yml'
    
    if config_file.exists():
        return BatchConfig.from_yaml(str(config_file))
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®š
        return BatchConfig.from_env()

# ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
Config = BatchConfig

def get_scheduling_config():
    """æ—¢å­˜ã®get_config()ã‚’æ´»ç”¨ã—ã¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¨­å®šã‚’å–å¾—
    
    âš ï¸  GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã®åŒæœŸ:
    - GitHub Actions: cron '0 */2 * * *' (2æ™‚é–“ã”ã¨)
    - ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ: status_collection_interval: 120åˆ†ã§çµ±ä¸€
    """
    config = get_config()
    config_dict = config.to_dict()
    scheduling = config_dict.get('scheduling', {})
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’120åˆ†ï¼ˆ2æ™‚é–“ï¼‰ã«è¨­å®šã—ã¦GitHub Actionsã¨çµ±ä¸€
    interval = scheduling.get('status_collection_interval', 120)
    
    return {
        'status_cron': f"*/{interval} * * * *",
        'working_rate_cron': f"{scheduling.get('history_calculation_minute', 0)} {scheduling.get('history_calculation_hour', 12)} * * *",
        'timezone': 'Asia/Tokyo'
    }

def get_database_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã‚’å–å¾—"""
    try:
        # secret.ymlã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿
        project_root = Path(__file__).parent.parent.parent
        secret_file = project_root / 'config' / 'secret.yml'
        
        if secret_file.exists():
            with open(secret_file, 'r', encoding='utf-8') as f:
                secret_data = yaml.safe_load(f)
                database_config = secret_data.get('database', {})
                
                if database_config.get('url'):
                    return DatabaseConfig(
                        connection_string=database_config['url'],
                        password=database_config.get('password', ''),
                        url=database_config['url']
                    )
                else:
                    raise ValueError("secret.ymlã«database.urlãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç’°å¢ƒå¤‰æ•°
            return DatabaseConfig.from_env()
    except Exception as e:
        print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚secret.ymlã®database.urlã¾ãŸã¯DATABASE_URLç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç’°å¢ƒå¤‰æ•°
        return DatabaseConfig.from_env()

def get_scraping_config():
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®šã‚’å–å¾—"""
    try:
        project_root = Path(__file__).parent.parent.parent
        config_file = project_root / 'config' / 'config.yml'
        
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                config_dict = yaml.safe_load(f)
            
            scraping_config = config_dict.get('scraping', {})
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
            return {
                'timeout': scraping_config.get('timeout', 30),
                'retry_attempts': scraping_config.get('retry_attempts', 3),
                'user_agents': scraping_config.get('user_agents', [
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                ]),
                'max_parallel_businesses': scraping_config.get('max_parallel_businesses', 3),
                'min_delay': scraping_config.get('min_delay', 0.5),
                'max_delay': scraping_config.get('max_delay', 2.0),
                'request_interval': scraping_config.get('request_interval', 1.0),
                'retry_delay': scraping_config.get('retry_delay', 3.0),
                'use_aiohttp': scraping_config.get('use_aiohttp', True),
                'connection_pooling': scraping_config.get('connection_pooling', True),
                'keep_alive': scraping_config.get('keep_alive', True),
                'compress': scraping_config.get('compress', True)
            }
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            return {
                'timeout': 30,
                'retry_attempts': 3,
                'user_agents': [
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                ],
                'max_parallel_businesses': 3,
                'min_delay': 0.5,
                'max_delay': 2.0,
                'request_interval': 1.0,
                'retry_delay': 3.0,
                'use_aiohttp': True,
                'connection_pooling': True,
                'keep_alive': True,
                'compress': True
            }
    except Exception as e:
        print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return {
            'timeout': 30,
            'retry_attempts': 3,
            'user_agents': [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            ],
            'max_parallel_businesses': 1,  # å®‰å…¨ã®ãŸã‚ä¸¦è¡Œå‡¦ç†ã‚’ç„¡åŠ¹
            'min_delay': 1.0,
            'max_delay': 3.0,
            'request_interval': 2.0,
            'retry_delay': 5.0,
            'use_aiohttp': False,  # å®‰å…¨ã®ãŸã‚aiohttpç„¡åŠ¹
            'connection_pooling': False,
            'keep_alive': False,
            'compress': False
        }
